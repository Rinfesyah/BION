Memahami konteks dalam kalimat adalah tantangan besar bagi NLP, karena bahasa manusia sering kali mengandalkan konteks untuk membedakan makna. Model Transformer, yang diperkenalkan dalam publikasi terkenal pada tahun 2017, telah menjadi landasan utama untuk memahami konteks ini. Model Transformer menggunakan pendekatan yang dikenal sebagai self-attention, yang memungkinkan AI untuk mempertimbangkan hubungan antara kata-kata di seluruh teks, bahkan kata-kata yang terpisah jauh. Ini memungkinkan model untuk “mengingat” kata-kata penting yang mungkin muncul di awal teks ketika memproses kata-kata di akhir teks. Model seperti BERT (Bidirectional Encoder Representations from Transformers) dan GPT (Generative Pre-trained Transformer) dilatih untuk memahami konteks dengan cara bidirectional. Misalnya, BERT membaca teks dari depan ke belakang dan sebaliknya, untuk menangkap konteks lebih baik. Hal ini memungkinkan model untuk mengerti kata-kata berdasarkan kalimat secara keseluruhan, bukan hanya berdasarkan kata-kata di sekitar secara langsung.jaringan saraf tiruan mulai digunakan, menghasilkan kemajuan signifikan dalam kemampuan mesin untuk memahami dan menghasilkan bahasa. Model seperti Word2Vec memperkenalkan representasi kata berbasis vektor, meningkatkan pemahaman semantik dan sintaksis.